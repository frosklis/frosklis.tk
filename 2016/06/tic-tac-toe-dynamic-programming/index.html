<!doctype html><html lang=es-es><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Sitio web de Claudio Noguera"><title>Claudio Noguera</title><script src=https://kit.fontawesome.com/911291c64e.js crossorigin=anonymous></script><link rel=icon href=/favicon.png><link rel=stylesheet href=https://frosklis.tk/main.min.3df621cfdc214478d8b44439244ddcb325a0a4d8182c3af9712ec72f414642a6.css></head><body><nav class=sidebar><a href=https://frosklis.tk/><h1 class=tc>Claudio Noguera</h1></a><div><div class="ml5 mr5 mb2 ma1 tc"><a href=https://frosklis.tk/><img src=/claudio_salinas.jpg class=shadow-1 alt=logo></a></div><div class=tc><a class=ma2 href=https://github.com/frosklis><i class="fa-2x fab fa-github" aria-label=github title=github></i></a><a class=ma2 href=https://linkedin.com/in/claudionoguera><i class="fa-2x fab fa-linkedin" aria-label=linkedin title=linkedin></i></a><a class=ma2 href=https://twitter.com/claudio1985><i class="fa-2x fab fa-twitter" aria-label=twitter title=twitter></i></a></div></div><div class="copyright tc">2020
<a href=https://creativecommons.org/licenses/by/4.0/><i class="fa-lg fab fa-creative-commons"></i><i class="fa-lg fab fa-creative-commons-by"></i></a></div></nav><div id=content><article><header><h1>Tic-tac-toe dynamic programming</h1><div class=post-meta><i class="far fa-calendar"></i>2016-06-24</div><div class=post-meta><i class="fas fa-tag"></i><a href=/tags/reinforcement-learning>reinforcement learning</a>, <a href=/tags/dynamic-programming>dynamic programming</a></div></header><p>This is the first of many articles regarding reinforcement learning, which is something I want to master. I will try to do it using <a href=https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html>&ldquo;Reinforcement learning&rdquo;, by Richard Sutton and Andrew Barto</a> as a reference. The objective of a reinforcement learning system is building a program that learns rather than the much more usual, the human learns and teaches the computer the result of its learning.</p><p>As with everything, first steps are humble: the tic-tac-toe game. The book does not provide an implementation. I found however <a href=https://www.xanvong.com/posts/blog/tic-tac-toe-dp-and-memo>this nice implementation</a> using dynamic programming:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_best_responses</span>(self, board<span style=color:#f92672>=</span>None, player<span style=color:#f92672>=</span>None):
    <span style=color:#e6db74>&#34;&#34;&#34;Recursively compute best responses for all subgames of current board.
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>    Call with no arguments to build the entire best_responses dict.
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>    This adds a key to best_responses for each possible board configuration
</span><span style=color:#e6db74>    that could follow from board.
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>    player = 1 for X, -1 for O. This is the current player, i.e. the one
</span><span style=color:#e6db74>    who goes next given the current board.
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>

    <span style=color:#75715e># Initialize</span>
    <span style=color:#66d9ef>if</span> board <span style=color:#f92672>is</span> None:
        board <span style=color:#f92672>=</span> (<span style=color:#ae81ff>0</span>,) <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>SIZE
        player <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>

    <span style=color:#66d9ef>if</span> board <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>best_responses:
        <span style=color:#66d9ef>return</span>

    win <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>check_win(board, player)
    <span style=color:#75715e># If win/loss/draw has been determined, the game is over.</span>
    <span style=color:#66d9ef>if</span> win <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
        self<span style=color:#f92672>.</span>best_responses[board] <span style=color:#f92672>=</span> (None, win) <span style=color:#75715e># None =&gt; no move needed</span>
        <span style=color:#66d9ef>return</span>

    <span style=color:#75715e># If we don&#39;t know the best response yet, compute it.</span>
    best_value <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>
    <span style=color:#66d9ef>for</span> i, val <span style=color:#f92672>in</span> enumerate(board):
        <span style=color:#66d9ef>if</span> val <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
            <span style=color:#75715e># create new tuple with player&#39;s move in ith slot</span>
            board2 <span style=color:#f92672>=</span> board[:i] <span style=color:#f92672>+</span> (player,) <span style=color:#f92672>+</span> board[i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>:]

            <span style=color:#75715e># If board2 is already in best_responses, this does nothing.</span>
            <span style=color:#75715e># Otherwise, it ensures board2 is added to best_responses.</span>
            self<span style=color:#f92672>.</span>build_best_responses(board2, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> <span style=color:#f92672>*</span> player)
            <span style=color:#75715e># The player&#39;s value given board2 is the reverse of the next</span>
            <span style=color:#75715e># player&#39;s value</span>
            value <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>best_responses[board2][<span style=color:#ae81ff>1</span>]
            <span style=color:#66d9ef>if</span> value <span style=color:#f92672>&gt;</span> best_value:
                best_value, best_move <span style=color:#f92672>=</span> value, i
    self<span style=color:#f92672>.</span>best_responses[board] <span style=color:#f92672>=</span> (best_move, best_value)
</code></pre></div><p>Is this reinforcement learning? What this code does is for any board composition it computes the value of the board, and it starts with the empty board. So effectively it is enumerating all the possible states, something that can be done for simple games like tic-tac-toe.</p><p>Being able to evaluate all posible states makes it possible to evaluate the game in a deterministic way, which makes finding the best policy easy. The difficulty of reinforcement learning, I guess, is determining this &ldquo;board value&rdquo;, it will not always be simple.</p></article></div><footer><div class="copyright tc">2020
<a href=https://creativecommons.org/licenses/by/4.0/><i class="fa-lg fab fa-creative-commons"></i><i class="fa-lg fab fa-creative-commons-by"></i></a></div></footer></body></html>